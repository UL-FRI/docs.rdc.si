{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"News","text":"<p>As part of its strategy to support its researchers in their endeavors University of Ljubljana, Faculty of Computer and Information Science (UL FRI) actively invests in research infrastructure. In the summer of 2023, one such investment was the expansion of an existing, homegrown Slurm cluster with additional hardware, and the acquirement of servers to be specifically dedicated to inferencing. These two clusters are named FRIDA and FRIKA, respectively.</p> <p>Delivery of new modular CDC, on-site integration &amp; preflight testing</p> 2025-Q3 <p>FRIDA WEKA storage goes live</p> 2025-M3 <p>Construction of new modular CDC</p> 2025 <p>Contract for new modular CDC signed</p> 2025-M1 <p>Tenders for new modular CDC open</p> 2024-H2 <p>FRIDA nxt &amp; amd partitions go live</p> 2024-M6 <p>Setup, deployment &amp; debugging of the FRIDA future tech partitions</p> 2024-Q2 <p>Planning of new modular CDC starts</p> 2024-M3 <p>Docs, Support desk setup, Monitoring, Teleport, onboarding of first users</p> 2024-Q1 <p>FRIDA &amp; FRIKA go live</p> 2024-M1 <p>Setup, deployment &amp; debugging</p> 2023-Q4"},{"location":"FRIDA/about/","title":"About FRIDA","text":"<p>FRIDA is a Slurm cluster that was initially set up with the help of the Development of Slovene in the Digital Environment (RSDO) project in 2020 when the first NVIDIA DGX-A100 system was acquired. Later it was upgraded with a new and more capable login node, and two compute nodes with 8 80GB/GPU GPUs each, one of them being an NVIDIA DGX-H100.</p> <p>FRIDA is progressively further expanded. For example in Q2 of 2024, attention was given to future/alternative technologies, with the addition of development kit compute nodes equipped with AMD MI210 GPUs as well as NVIDIA GraceHopper Superchips. Later expansions concentrated on faster InfiniBand data interconnect, a fast and larger shared data storage based on the Weka Data Platform. The most recent addition is a NVIDIA DGX-B200 with 8 180GB/GPU Blackwell GPUs that are interconnected with the 1.8TB/s 5th gen NVLink+NVSwitch.</p> <p>Research labs that own and manage their computing systems themselves, may inquire about the possibility of integrating their infrastructure into FRIDA. Cofunding of future FRIDA expansions is also possible. All inquiries should be addressed to the UL FRI Management Board, the technical details will be coordinated by the FRIDA technical committee.</p>"},{"location":"FRIDA/access/","title":"Obtaining an account","text":"<p>Access to the FRIDA compute infrastructure is granted (upon request) to all employees of UL FRI. Under the guidance and responsibility of a UL FRI employee access can be granted also to students who cooperate on research lab projects, or require access to the infrastructure to work on their doctoral, master's or diploma theses. The access request must come from a UL FRI employee (supervisor), stating the research lab, project, and/or thesis topic, as well as the duration for which access is requested.</p> <p>Requests for access, as well as technical questions, should be directed to frida@rdc.si.</p>"},{"location":"FRIDA/access/#stale-accounts","title":"Stale accounts","text":"<p>Usage of FRIDA is monitored to ensure fair sharing of the available resources. Stale accounts, i.e. accounts that did not submit any jobs in the last 6 months, are automatically disabled, and the account-associated data is archived. If the account was the last account of a specific research lab, group, or project, the corresponding research lab, group, or project data is archived as well.</p> <p>The archived data is kept for 3 additional months to facilitate an eventual reenablement of the account or download of the associated data upon request. After this grace period, all data is permanently deleted.</p>"},{"location":"FRIDA/access/#passwordless-access","title":"Passwordless access","text":"<p>In line with the UL security policies, access to FRIDA is strictly multi-factor authentication (MFA) based. To achieve the most seamless integration and provide the best possible user experience, FRIDA opted for Teleport (see How Teleport Works) and the corresponding <code>tsh</code> Command Line Interface (CLI) client (see Using the tsh Command Line Tool). Teleport works via HTTPS tunneling, so you can also use it from any restricted networks that prohibit normal SSH connections (port 22/TCP).</p> <p>Tip</p> <p>For the best user experience, we suggest users set up their accounts for passwordless access via Apple Touch ID, Windows Hello or Yubikey BIO. A less user-friendly alternative is 2FA via hardware token (e.g. Youbikey 5C Nano) or OTP (e.g. Raivo or some other OTP Authenticator App).</p>"},{"location":"FRIDA/access/#registration","title":"Registration","text":"<p>Once granted access to your request, you will receive a Teleport signup link. Open the link, click on Use password and follow the instructions to register a password and OTP device. You can also register a hardware key (Apple Touch ID, Windows Hello, or Yubikey) for passwordless web access.</p> <p>Warning</p> <p>Registration of a password and OTP device during the registration process is mandatory, otherwise you will later not be able to register a hardware key for CLI access.</p> <p>Once the registration is finished, follow the official Teleport instructions to install the Teleport Community Edition of the <code>tsh</code> CLI client appropriate to your OS (macOS, Windows, or Linux). Ensure that the <code>tsh</code> client install location is included in your <code>PATH</code> variable. If you wish to do so, you can, as an addition, install Teleport Connect (a Graphical User Interface (GUI) desktop client), but for most use cases this is not necessary.</p> <p>Warning</p> <p>TouchID support on macOS is now included the main package. Support can be checked by running <code>tsh touchid diag</code>.</p> <p>Now you can register your hardware key (Apple Touch ID, Windows Hello, or Yubikey BIO) to enable passwordless infrastructure access also via CLI. To do so, you execute the following commands in your terminal. For successful registration, you will need to provide your password and second-factor key (OTP). <pre><code>$ tsh --proxy=rdc.si --user={username} --mfa-mode=otp --auth=local login\nEnter password for Teleport user {username}:\nEnter an OTP code from a device:\n&gt; Profile URL:        https://rdc.si:443\n  Logged in as:       {username}\n  Cluster:            rdc.si\n  Roles:              access_app_grafana_frida, access_frida\n  Logins:             {username}\n  Kubernetes:         enabled\n  Valid until:        2024-02-14 19:35:00 +0100 CET [valid for 8h0m0s]\n  Extensions:         login-ip, permit-agent-forwarding, permit-port-forwarding, permit-pty, private-key-policy\n\nDid you know? Teleport Connect offers the power of tsh in a desktop app.\nLearn more at https://goteleport.com/docs/connect-your-client/teleport-connect/\n\n$ tsh --proxy=rdc.si --user={username} --mfa-mode=otp --auth=local mfa add --type=TOUCHID --name=touchid.cli\nEnter an OTP code from a *registered* device:\nUsing platform authenticator, follow the OS prompt\nMFA device \"touchid.cli\" added.\n</code></pre></p> <p>Note</p> <p>In contrast to Windows Hello and Yubikey, you will need to register your Apple TouchID multiple times: once for CLI access, and once for every browser that you wish to use. This is by design, a security requirement of Apple TouchID.</p>"},{"location":"FRIDA/access/#first-access","title":"First access","text":"<p>Use the <code>tsh</code> client to authenticate yourself to the FRIDA Teleport gateway <pre><code>$ tsh --proxy=rdc.si --user={username} login\n</code></pre></p> <p>Note</p> <p>FRIDA is configured to be passwordless first, meaning, in case you do not enable passwordless CLI access, you must add <code>--mfa-mode=otp --auth=local</code> to all <code>tsh --proxy=rdc.si</code> commands.</p> <p>Upon successful authentication create or edit the SSH config file corresponding to your OS (<code>~/.ssh/config</code> on Unix-like, <code>%userprofile%/.ssh/config</code> on Windows systems), and add to the top the snippet that you obtain by running the following command. <pre><code>$ tsh config\n</code></pre></p> <p>While doing so, add also the following lines to the end of the section marked as <code># Common flags for all ... hosts</code>. These lines are not mandatory but will reduce the number of times that you need to reauthenticate yourself, as well as, use a forwarding agent to automatically forward your locally stored SSH keys so that you can access other machines with your private SSH keys. <pre><code>ServerAliveInterval 300\nServerAliveCountMax 2\nTCPKeepAlive yes\nForwardAgent yes\nAddKeysToAgent yes\n</code></pre></p> <p>Save the SSH config and you are set to go. From now on you can interact with FRIDA by running standard <code>ssh</code> and <code>scp</code> commands.</p>"},{"location":"FRIDA/access/#vscode-and-remote-ssh-access-to-the-login-node","title":"VSCode and Remote SSH access to the login node","text":"<p>Visual Studio Code's Remote SSH extension (part of the Remote Development Extension pack) in the Remote Explorer does not list wildcard-based hosts declared in the SSH config file. To have the FRIDA login node available in Remote Explorer you must add it to the SSH config file explicitly.</p> macOSLinuxWin <pre><code># FRIDA login host for VSCode Remote SSH\nHost login-frida\n    Hostname login-frida.rdc.si\n    User {username}\n    # copy from '# Common flags for all rdc.si hosts'\n    UserKnownHostsFile \"/Users/{username}/.tsh/known_hosts\"\n    IdentityFile \"/Users/{username}/.tsh/keys/rdc.si/{username}\"\n    CertificateFile \"/Users/{username}/.tsh/keys/rdc.si/{username}-ssh/rdc.si-cert.pub\"\n    HostKeyAlgorithms rsa-sha2-512-cert-v01@openssh.com,rsa-sha2-256-cert-v01@openssh.com,ssh-rsa-cert-v01@openssh.com\n    # copy from '# Flags for all rdc.si hosts except the proxy'\n    Port 3022\n    ProxyCommand \"/usr/local/bin/tsh\" proxy ssh --cluster=rdc.si --proxy=rdc.si:443 %r@%h:%p\n    # copy from '# Flags for all rdc.si hosts except the proxy', optional lines that were appended during First access\n    ServerAliveInterval 300\n    ServerAliveCountMax 2\n    TCPKeepAlive yes\n    ForwardAgent yes\n    AddKeysToAgent yes\n</code></pre> <pre><code># FRIDA login host for VSCode Remote SSH\nHost login-frida\n    Hostname login-frida.rdc.si\n    User {username}\n    # copy from '# Common flags for all rdc.si hosts'\n    UserKnownHostsFile \"/home/{username}/.tsh/known_hosts\"\n    IdentityFile \"/home/{username}/.tsh/keys/rdc.si/{username}\"\n    CertificateFile \"/home/{username}/.tsh/keys/rdc.si/{username}-ssh/rdc.si-cert.pub\"\n    HostKeyAlgorithms rsa-sha2-512-cert-v01@openssh.com,rsa-sha2-256-cert-v01@openssh.com,ssh-rsa-cert-v01@openssh.com\n    # copy from '# Flags for all rdc.si hosts except the proxy'\n    Port 3022\n    ProxyCommand \"/usr/local/bin/tsh\" proxy ssh --cluster=rdc.si --proxy=rdc.si:443 %r@%h:%p\n    # copy from '# Flags for all rdc.si hosts except the proxy', optional lines that were appended during First access\n    ServerAliveInterval 300\n    ServerAliveCountMax 2\n    TCPKeepAlive yes\n    ForwardAgent yes\n    AddKeysToAgent yes\n</code></pre> <p>Make sure that <code>tsh.exe</code> is on your PATH.</p> <pre><code># FRIDA login host for VSCode Remote SSH\nHost login-frida\n    Hostname login-frida.rdc.si\n    User {username}\n    # copy from '# Common flags for all rdc.si hosts'\n    UserKnownHostsFile \"C:\\Users\\{username}\\.tsh\\known_hosts\"\n    IdentityFile \"C:\\Users\\{username}\\.tsh\\keys\\rdc.si\\{username}\"\n    CertificateFile \"C:\\Users\\{username}\\.tsh/keys\\rdc.si\\{username}-ssh\\rdc.si-cert.pub\"\n    HostKeyAlgorithms rsa-sha2-512-cert-v01@openssh.com,rsa-sha2-256-cert-v01@openssh.com,ssh-rsa-cert-v01@openssh.com\n    # copy from '# Flags for all rdc.si hosts except the proxy'\n    Port 3022\n    ProxyCommand tsh proxy ssh --cluster=rdc.si --proxy=rdc.si:443 %r@%h:%p\n    # copy from '# Flags for all rdc.si hosts except the proxy', optional lines that were appended during First access\n    ServerAliveInterval 300\n    ServerAliveCountMax 2\n    TCPKeepAlive yes\n    ForwardAgent yes\n    AddKeysToAgent yes\n</code></pre> <p>Note</p> <p>To be compatible with Teleport connections, Visual Studio Code needs to be configured properly, i.e. the <code>Remote.SSH: Use Local Server</code> config setting must be disabled. For details see the official Teleport documentation.</p>"},{"location":"FRIDA/slurm/","title":"Slurm","text":"<p>Reservation and management of FRIDA compute resources is based on Slurm (Simple Linux Utility for Resource Management), a software package for submitting, scheduling, and monitoring jobs on large compute clusters. Slurm governs access to the cluster's resources through three views; account/user association, partition, and QoS (Quality of Service). Accounts link users into groups (based on research lab, project or other shared properties). Partitions link nodes and QoS allow users to control the priority and limits on a job level. Each of these views can impose limits and affect the submitted job's priority.</p>"},{"location":"FRIDA/slurm/#nodes","title":"Nodes","text":"<p>FRIDA currently consists of one login node and several compute nodes with characteristics listed in the table below. The total compute currently consists of 1456 vCPUs with 8576GB of system RAM, and 40 GPUs with 3392GB GPU RAM. Although the login node has Python 3.10 and venv pre-installed, these are provided only to aid in quick scripting, and the login node is not intended for any intensive processing. Refrain from installing user space additions like conda and others. All computationally intensive tasks must be submitted as Slurm jobs via the corresponding Slurm commands. User accounts that fail to adhere to these guidelines will be subject to suspension.</p> NODE ROLE vCPU MEM nGPU GPU type login login 64 256GB - - aga compute 256 512GB 4 NVIDIA A100 40GB SXM4 apl compute 112 1TB 8 NVIDIA L4 24GB PCIe ana compute 112 1TB 8 NVIDIA A100 80GB PCIe ixh compute 224 2TB 8 NVIDIA H100 80GB HBM3 ixb compute 224 2TB 8 NVIDIA B200 180GB gh[1-2] compute 72 576GB 1 NVIDIA GH200 480GB api compute 384 768GB 2 AMD MI210 64GB PCIe <p>The compute node naming scheme follows a two/three letter acronym that is based on the node architecture and suffixed by a number, if multiple such nodes exist. For example node name <code>ana</code> stands for AMD CPU, NVLink interconnect, and Ampere GPU, <code>api</code> stands for AMD CPU, PCIe interconnect, and AMD Instinct MIxxx GPU, <code>aga</code> stands for AMD CPU, gang NVLink (gen 3) interconnect (i.e. no NVSwitch), Ampere GPU, <code>ixh</code> stands for Intel CPU, SXM5 - NVLink + NVSwitch (gen 4) interconnect, and Hopper GPU, Ampere GPU, <code>ixb</code> stands for Intel CPU, SXM6 - NVLink + NVSwitch (gen 5) interconnect, and Blackwell GPU, and <code>gh</code> stands for GraceHopper superchip (i.e. Grace CPU and Hopper GPU tightly bound via a 900GB/s NVLink-C2C interconnect, allowing for cache and memory coherency). More details about the nodes can be obtaind by executing the command <code>scontrol show node &lt;node_name&gt;</code>; note that each node is also assigned a series of features which can be used in conjuction with parameter <code>-C/--constraint &lt;key&gt;:&lt;value&gt;</code> to target a specific node.</p> NODE CPU_BRD CPU_GEN CPU_SKU CPU_L3 CPU_MEM GPU_BRD GPU_GEN GPU_SKU GPU_MEM GPU_CC aga AMD ZEN3 EPYC_7763 256MB 512GB NVIDIA AMPERE A100_SXM4_40GB 40GB 8.0 apl AMD ZEN3 EPYC_7453 64MB 1TB NVIDIA ADA_LOVELACE L4_24GB_PCIE 24GB 8.9 ana AMD ZEN3 EPYC_7453 64MB 1TB NVIDIA AMPERE A100_80GB_PCIE 80GB 8.0 ixh INTEL GOLDEN_COVE PLATINUM_8480CL 105MB 2TB NVIDIA HOPPER H100_80GB_HBM3 80GB 9.0 ixb INTEL EMERALD_RAPIDS PLATINUM_8570 200MB 2TB NVIDIA BLACKWELL B200_180GB 180GB 10.0 gh[1-2] ARM NEO2 GRACE 234MB 576GB NVIDIA HOPPER GH200_480GB 96GB 9.0 api AMD ZEN4 EPYC_9684X 1152MB 768GB AMD CDNA2 MI210 64GB -"},{"location":"FRIDA/slurm/#partitions","title":"Partitions","text":"<p>Within Slurm subsets of compute nodes are organized into partitions. On FRIDA there are two types of partitions, general and private (available to selected research labs or groups based on their co-funding of FRIDA). Interactive jobs can be run only on partition <code>dev</code>. Production runs are not permitted in interactive jobs, <code>dev</code> partition is thus intended to be used for code development, testing, and debugging only.</p> PARTITION TYPE nodes default time max time available gres types frida general all* 4h 7d gpu, gpu:L4, gpu:A100, gpu:A100_80GB, gpu:H100, gpu:B200 dev general aga,ana,apl 2h 12h gpu, gpu:L4, gpu:A100, gpu:A100_80GB cjvt private axa 4h 4d gpu, gpu:A100 psuiis private ana 4h 4d gpu, gpu:A100_80GB nxt experimental gh[1-2] 2h 2d gpu, gpu:GH200 amd experimental api 2h 2d gpu, gpu:MI210 <p>Note</p> <p>To avoid issues related to differences in CPU and/or GPU architecture, partition <code>frida</code> includes all nodes, but those that are part of partitions <code>nxt</code> and <code>amd</code>.</p> <p>Tip</p> <p>Jobs that are CPU intense should target partition <code>amd</code> as it is equipped with large cache AMD Epyc 9684X CPUs.</p>"},{"location":"FRIDA/slurm/#shared-storage","title":"Shared storage","text":"<p>All FRIDA nodes have access to a limited amount of shared data storage. For performance reasons, it is served by a raid0 backend. Note that FRIDA does not provide automatic backups, these are entirely in the domain of the end user. Also, as of current settings, FRIDA does not enforce shared storage quotas, but this may change in future upgrades. Access permissions on the user folder are set to user only. Depending on the groups a user is a member of, they may have access to multiple workspace folders. These folders have the group special (SGID) bit set, so files created within them will automatically have the correct group ownership. All group members will have read and execute rights. Note, however, that group write access is masked, so users who wish to make their files writable by other group members should change permissions by using the <code>chmod g+w</code> command. All other security measures dictated by the nature of your data are the responsibility of the end users.</p> <p>In addition to access to shared storage, compute nodes provide also an even smaller amount of local storage. The amount varies per node and may change with FRIDA's future updates. Local storage is intended as scratch space, the corresponding path is created on a per-job basis at job start and purged as soon as the job ends.</p> TYPE backend backups access location env quota shared weka no user <code>/shared/home/$USER</code> <code>$HOME</code> - shared weka no group <code>/shared/workspace/$SLURM_JOB_ACCOUNT</code> <code>$WORK</code> - scratch raid0 no job <code>/local/scratch/$USER/$SLURM_JOB_ID</code> <code>$SCRATCH</code> -"},{"location":"FRIDA/slurm/#usage","title":"Usage","text":"<p>In Slurm there are two principal ways of submitting jobs. Interactive and non-interactive, or batch submission. What follows in the next subsections is a quick review of most typical use cases.</p> <p>Some useful Slurm commands with their typical use case, notes and corresponding Slurm help are displayed in the table below. In the next sections, we will be using these commands to submit and view jobs.</p> CMD typical use case notes help <code>frida</code> view the current state of resources this is a custom, FRIDA command, which in a compact form presents important FRIDA notices about maintenance; it also prints detailed info of the last five jobs submitted by the user, a list of their shared storage locations with usage and remaining space, and a list of currently available GPUs <code>slurm</code> view the current status of all resources this is a custom, FRIDA alias built on top of the more general <code>sinfo</code> Slurm command see Slurm docs on <code>sinfo</code> <code>salloc</code> resource reservation and allocation for interactive use intended for interactive jobs; upon successful allocation <code>srun</code> commands can be used to open a shell with the allocated resources see Slurm docs on <code>salloc</code> <code>srun</code> resource reservation, allocation and execution of supplied command on these resources a blocking command; with <code>--pty</code> can be used for interactive jobs; by appending <code>&amp;</code> followed by a <code>wait</code> the command can be turned into non-blocking see Slurm docs on <code>srun</code> <code>stunnel</code> resource reservation, allocation and setup of a vscode tunnel to these resources this is a custom, FRIDA alias built on top of the more general <code>srun --pty</code> Slurm command; requires a GitHub or Microsoft account for tunnel registration; intended for dvelopment, testing and debugging see section code tunnel <code>sbatch</code> resource reservation, allocation and execution of non-interactive batch job on these resources asynchronous execution of sbatch script; when combined with <code>srun</code> multiple sub-steps become possible see Slurm docs on <code>sbatch</code> <code>squeue</code> view the current queue status on large clusters the output can be large; filter can be applied to limit output to specific partition by <code>-p {partition}</code> see Slurm docs on <code>squeue</code> <code>sprio</code> view the current priority status on queue this is a custom FRIDA alias built on top of the more general <code>sprio</code> Slurm command see Slurm docs on <code>sprio</code> <code>scancel</code> cancel a running job see Slurm docs on <code>scancel</code> <code>scontrol show job</code> show detailed information of a job see Slurm docs on <code>scontrol</code>"},{"location":"FRIDA/slurm/#interactive-sessions","title":"Interactive sessions","text":"<p>Slurm in general provides two ways of running interactive jobs; either via the <code>salloc</code> or via the <code>srun --pty</code> command. The former will first reserve the resources, which you then explicitly use via the latter, while the latter can be used to do the reservation and execution in a single step.</p> <p>For example, the following snippet shows how to allocate resources on <code>dev</code> partition with default parameters (2 vCPU, 8GB RAM) and then start a bash shell on the allocated resources. Done that it shows how to check what was allocated, then exits the bash shell and finally releases the allocated resources. <pre><code>ilb@login-frida:~$ salloc -p dev\nsalloc: Granted job allocation 498\nsalloc: Waiting for resource configuration\nsalloc: Nodes ana are ready for job\n\nilb@login-frida:~ [interactive]$ srun --pty bash\n\nilb@ana-frida:~ [interactive]$ scontrol show job $SLURM_JOB_ID | grep AllocTRES\n   AllocTRES=cpu=2,mem=8G,node=1,billing=2\n\nilb@ana-frida:~ [interactive]$ exit\nexit\n\nilb@login-frida:~ [interactive]$ exit\nexit\nsalloc: Relinquishing job allocation 498\nsalloc: Job allocation 498 has been revoked.\n\nilb@login-frida:~$\n</code></pre></p> <p>The following snipped, on the other hand, allocates resources on <code>dev</code> partition and starts a bash shell in one call, this time requesting for 32 vCPU cores and 32 GB of RAM. It then checks what was allocated, and finally exits from the bash shell jointly releasing the allocated resources. <pre><code>ilb@login-frida:~$ srun -p dev -c32 --mem=32G --pty bash\n\nilb@ana-frida:~ [bash]$ scontrol show job $SLURM_JOB_ID | grep AllocTRES\n   AllocTRES=cpu=32,mem=32G,node=1,billing=32\n\nilb@ana-frida:~ [bash]$ exit\nexit\n\nilb@login-frida:~$\n</code></pre></p> <p>Access to the allocated resources (nodes) can be achieved also by invoking <code>ssh</code>; however, note that then the Slurm-defined environment variables will not be available to you. The auto inclusion of the corresponding environment variables is an added benefit of accessing the allocated resources via <code>srun --pty</code>. With multi-node resource allocations use <code>-w</code> to specify the node on which you wish to start a shell. <pre><code>ilb@login-frida:~$ srun -p dev --pty bash\n\nilb@ana-frida:~ [bash]$ echo $SLURM_\n$SLURM_CLUSTER_NAME       $SLURM_JOB_ID             $SLURM_JOB_QOS            $SLURM_SUBMIT_HOST\n$SLURM_CONF               $SLURM_JOB_NAME           $SLURM_NNODES             $SLURM_TASKS_PER_NODE\n$SLURM_JOBID              $SLURM_JOB_NODELIST       $SLURM_NODELIST\n$SLURM_JOB_ACCOUNT        $SLURM_JOB_NUM_NODES      $SLURM_NODE_ALIASES\n$SLURM_JOB_CPUS_PER_NODE  $SLURM_JOB_PARTITION      $SLURM_SUBMIT_DIR\n\nilb@ana-frida:~ [bash]$ exit\nexit\n\nilb@login-frida:~$\n</code></pre></p> <p>Note also that when you have multiple jobs running on a single node <code>ssh</code> will always open a shell with the resources allocated by the last submitted job. The <code>srun</code> command on the other hand provides two parameters through which one can specify the desire to not start a new job, but open an additional shell in an already running job. For example, assuming a job with id <code>500</code> is already running, the next snippet shows how to open a second bash shell in the same job. <pre><code>ilb@login-frida:~$ srun --overlap --jobid=500 --pty bash\n\nilb@ana-frida:~ [bash]$ echo $SLURM_JOB_ID\n500\n\nilb@ana-frida:~ [bash]$ exit\nexit\n\nilb@login-frida:~$\n</code></pre></p>"},{"location":"FRIDA/slurm/#running-jobs-in-containers","title":"Running jobs in containers","text":"<p>FRIDA was set up by following deepops, an open-source infrastructure automation tool by NVIDIA. Our setup is focused on supporting ML/AI training tasks, and based on current trends of containerization even in HPC systems, it is also purely container-based (modules are not supported). Here we opted for Enroot an extremely lightweight container runtime capable of turning traditional container/OS images into unprivileged sandboxes. An added benefit of Enroot is its tight integration into Slurm commands via the Pyxis plugin, thus providing a very good user experience.</p> <p>Regardless if the Enroot/Pyxis bundle turns containers/OS images into unprivileged sandboxes, automatic root remapping allows users to extend and update the imported images with ease. The changes can be retained for the duration of a job or exported to a squashfs file for cross-job retention. Some of the most commonly used Pyxis-provided <code>srun</code> parameters are listed in the table below. For further details on all available parameters consult the official Pyxis documentation, while information on how to set up credentials that enable importing containers from private container registries can be found in the official Enroot documentation.</p> srun parameter format notes <code>--container-image</code> <code>[USER@][REGISTRY#]IMAGE[:TAG]\\|PATH</code> container image to load, path must point to a squashfs file <code>--container-name</code> <code>NAME</code> name the container for easier access to running containters; can be used to change container contents without saving to a squashfs file, but note that non-running named containers are kept only within a <code>salloc</code> or <code>sbatch</code> allocation <code>--container-save</code> <code>PATH</code> save the container state to a squashfs file <code>--container-mount-home</code> bind mount the user home; not mounted by default to prevent local config collisions <code>--container-mounts</code> <code>SRC:DST[:FLAGS][,SRC:DST...]</code> bind points to mount into the container; <code>$SCRATCH</code> is auto-mounted <p>For example, the following snippet will check the OS release on <code>dev</code> partition, node <code>ana</code>, then start a bash shell in a CentOS container on the same node and recheck. In this case, the CentOS image tagged latest will be pulled from the official DockerHub page. <pre><code>ilb@login-frida:~$ srun -p dev -w ana bash -c 'echo \"$HOSTNAME: `grep PRETTY /etc/os-release`\"'\nana: PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n\nilb@login-frida:~$ srun -p dev -w ana --container-image=centos --pty bash\npyxis: importing docker image: centos\npyxis: imported docker image: centos\n[root@ana /]# echo \"$HOSTNAME: `grep PRETTY /etc/os-release`\"\nana: PRETTY_NAME=\"CentOS Linux 8\"\n\n[root@ana /]# exit\nexit\n\nilb@login-frida:~$\n</code></pre></p> <p>The following snippet allocates resources on <code>dev</code> partition, then starts an <code>ubuntu:20.04</code> container and checks if <code>file</code> utility exists. The command results in an error indicating that the utility is not present. The snippet then creates a named container that is based on <code>ubuntu:20.04</code> and installs the utility. Then it rechecks if the utility exists in the named container. The snippet ends by releasing the resources, which jointly purges the named container. <pre><code>ilb@login-frida:~$ salloc -p dev\nsalloc: Granted job allocation 526\nsalloc: Waiting for resource configuration\nsalloc: Nodes ana are ready for job\n\nilb@login-frida:~ [interactive]$ srun --container-image=ubuntu:20.04 which file\npyxis: importing docker image: ubuntu:20.04\npyxis: imported docker image: ubuntu:20.04\nsrun: error: ana: task 0: Exited with exit code 1\n\nilb@login-frida:~ [interactive]$ srun --container-image=ubuntu:20.04 --container-name=myubuntu sh -c 'apt-get update &amp;&amp; apt-get install -y file'\npyxis: importing docker image: ubuntu:20.04\npyxis: imported docker image: ubuntu:20.04\nGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\nGet:3 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3238 kB]\nGet:4 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]\nGet:5 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3079 kB]\nGet:6 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1143 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\nGet:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3726 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3228 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.0 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1439 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\nFetched 29.4 MB in 3s (9686 kB/s)\nReading package lists...\nReading package lists...\nBuilding dependency tree...\nReading state information...\nThe following additional packages will be installed:\n  libmagic-mgc libmagic1\nThe following NEW packages will be installed:\n  file libmagic-mgc libmagic1\n0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 317 kB of archives.\nAfter this operation, 6174 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic-mgc amd64 1:5.38-4 [218 kB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic1 amd64 1:5.38-4 [75.9 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal/main amd64 file amd64 1:5.38-4 [23.3 kB]\ndebconf: delaying package configuration, since apt-utils is not installed\nFetched 317 kB in 1s (345 kB/s)\nSelecting previously unselected package libmagic-mgc.\n(Reading database ... 4124 files and directories currently installed.)\nPreparing to unpack .../libmagic-mgc_1%3a5.38-4_amd64.deb ...\nUnpacking libmagic-mgc (1:5.38-4) ...\nSelecting previously unselected package libmagic1:amd64.\nPreparing to unpack .../libmagic1_1%3a5.38-4_amd64.deb ...\nUnpacking libmagic1:amd64 (1:5.38-4) ...\nSelecting previously unselected package file.\nPreparing to unpack .../file_1%3a5.38-4_amd64.deb ...\nUnpacking file (1:5.38-4) ...\nSetting up libmagic-mgc (1:5.38-4) ...\nSetting up libmagic1:amd64 (1:5.38-4) ...\nSetting up file (1:5.38-4) ...\nProcessing triggers for libc-bin (2.31-0ubuntu9.12) ...\n\n\nilb@login-frida:~ [interactive]$ srun --container-name=myubuntu which file\n/usr/bin/file\n\nilb@login-frida:~ [interactive]$ exit\nexit\nsalloc: Relinquishing job allocation 526\nsalloc: Job allocation 526 has been revoked.\n\nilb@login-frida:~$\n</code></pre></p>"},{"location":"FRIDA/slurm/#start-a-second-shell-inside-the-same-container","title":"Start a second shell inside the same container","text":"<p>Using named containers becomes handy on occasions when one needs to open a second shell inside the same container (within the same job with the same resources). Let's assume a job with id <code>527</code> that was created with the parameter <code>--container-name=myubuntu</code> is running. Let's also assume that the <code>file</code> utility has been installed. The next snippet shows how to open a second bash shell into the same container. <pre><code>ilb@login-frida:~$ srun --overlap --jobid=527 --container-name=myubuntu --pty bash\n\nroot@ana:/# which file\n/usr/bin/file\n\nroot@ana:/# exit\nexit\n\nilb@login-frida:~$\n</code></pre></p> <p>Without the use of named containers, the task becomes more challenging as one needs to first start an overlapping shell on the node, find out the PID of the Enroot container in question, and then use Enroot commands to start a bash shell in that container. The following snippet shows an example. <pre><code>ilb@login-frida:~$ srun --overlap --jobid=527 --pty bash\n\nilb@ana-frida:~ [bash]$ enroot list -f\nNAME               PID    COMM  STATE  STARTED  TIME   MNTNS       USERNS      COMMAND\npyxis_527_527.0  11229  bash  Ss+    20:35    00:23  4026538269  4026538268  /usr/bin/bash\n\nilb@ana-frida:~ [bash]$ enroot exec 11229 bash\n\nroot@ana:/# which file\n/usr/bin/file\n\nroot@ana:/# exit\nexit\n\nilb@login-frida:~$\n</code></pre></p>"},{"location":"FRIDA/slurm/#save-the-container-image-to-a-local-filesystem","title":"Save the container image to a local filesystem","text":"<p>Named containers work very well throughout a single sbatch allocation, but when the same scripts are run multiple times, downloading from online container registries may take too much time. With multi-node runs certain container registries may even throttle downloads (e.g. when a large number of nodes starts to download concurrently). Or simply the container is seen just as a starting point on which one builds (installs other dependencies). For such cases it is useful to create a local copy of the container via the parameter <code>--container-save</code>. For example, the following snippet shows this workflow on the earlier example with <code>file</code>. <pre><code>ilb@login-frida:~$ srun -p dev --container-image=ubuntu:20.04 --container-save=./ubuntu_with_file.sqfs sh -c 'apt-get update &amp;&amp; apt-get install -y file'\nsrun: job 7822 queued and waiting for resources\nsrun: job 7822 has been allocated resources\npyxis: importing docker image: ubuntu:20.04\npyxis: imported docker image: ubuntu:20.04\n...\npyxis: exported container pyxis_7822_7822.0 to ./ubuntu_with_file.sqfs\n\nilb@login-frida:~$ ls -alht ubuntu_with_file.sqfs\n-rw-r----- 1 ilb lpt 128M Oct 24 12:07 ubuntu_with_file.sqfs\n\nilb@login-frida:~$ srun -p dev --container-image=./ubuntu_with_file.sqfs which file\nsrun: job 7823 queued and waiting for resources\nsrun: job 7823 has been allocated resources\n/usr/bin/file\n</code></pre></p> <p>Warning</p> <p>The parameter <code>--container-save</code> expects a file path, providing a folder path may lead to data loss. The parameter <code>--container-image</code> by default assumes an online image name, so local files should be provided in absolute path format, or prepended with <code>./</code> in the case of relative path format.</p>"},{"location":"FRIDA/slurm/#private-container-registries","title":"Private container registries","text":"<p>Enroot uses credentials configured through <code>$HOME/.config/enroot/.credentials</code>. Because the Pyxis/Enroot bundle pulls containers on the fly during the Slurm job start, the credentials file needs to be accessible in a shared filesystem which all nodes can access at job start. The file format of the credentials file is the following.</p> $HOME/.config/enroot/.credentials<pre><code>machine &lt;hostname&gt; login &lt;username&gt; password &lt;password&gt;\n</code></pre> <p>For example, credentials for the NVIDIA NGC registry would look as follows.</p> <pre><code># NVIDIA GPU Cloud (both endpoints are required)\nmachine nvcr.io login $oauthtoken password &lt;token&gt;\nmachine authn.nvidia.com login $oauthtoken password &lt;token&gt;\n</code></pre> <p>For further details consult the Enroot documentation, which provides additional examples.</p>"},{"location":"FRIDA/slurm/#requesting-gpus","title":"Requesting GPUs","text":"<p>All FRIDA compute nodes provide GPUs, but they differ in provider, architecture, and available memory. In Slurm, a request for allocation of GPU resources can be done using either by specifying the <code>--gpus=[type:]{count}</code> or <code>--gres=gpu[:type][:{count}]</code> parameter, as shown in the snippet below. The available GPU types depend on the selected partition, whose available GPU types in turn depend on the nodes that constitute the partition (see section partitions).</p> <pre><code>ilb@login-frida:~$ srun -p dev --gres=gpu nvidia-smi -L\nGPU 0: NVIDIA L4 (UUID: GPU-42461b88-e6ff-2a6d-cad6-583f99df26d2)\n\nilb@login-frida:~$ srun -p dev --gpus=A100_80GB:4 nvidia-smi -L\nGPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-845e3442-e0ca-a376-a3de-50e4cb7fd421)\nGPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-7f73bd51-df5b-f0ba-2cf2-5dadbfa297e1)\nGPU 2: NVIDIA A100 80GB PCIe (UUID: GPU-d666e6d2-5265-29ae-9a6f-d8772807d34f)\nGPU 3: NVIDIA A100 80GB PCIe (UUID: GPU-fd552b1f-e767-edd6-5cc0-69514f1748d2)\n</code></pre>"},{"location":"FRIDA/slurm/#code-tunnel","title":"Code tunnel","text":"<p>When an interactive session is intended for code development, testing, and/or debugging, many a time it is desirable to work with a suitable IDE. The requirement of resource allocation via Slurm and the lack of any toolsets on bare-metal hosts might seem too much of an added complexity, but in reality, there is a very elegant solution by using Visual Studio Code in combination with the Remote Development Extension (via <code>code_tunnel</code>). Running <code>code_tunnel</code> will allow you to use VSCode to connect directly to the container that is running on the compute node that was assigned to your job. Combined with root remapping this has the added benefit of a user experience that feels like working with your own VM.</p> <p>On every run of <code>code_tunnel</code> you will need to register the tunnel with your GitHub or Microsoft account; this interaction requires the <code>srun</code> command to be run with the parameter <code>--pty</code>, and for this reason, a suitable alias command named <code>stunnel</code> was setup. Once registered you will be able to access the compute node (while <code>code_tunnel</code> is running) either in a browser by visiting https://vscode.dev or with a desktop version of VSCode via the Remote Explorer (part of the Remote Development Extension pack). In both cases, the 'Remotes (Tunnels/SSH)' list in the Remote Explorer pane should contain a list of tunnels to FRIDA (named <code>frida-{job-name}</code>) that you have registered and also denote which of these are currently online (with your job running). In the browser, it is also possible to connect directly to a workspace on the node (on which the <code>code_tunnel</code> job is running). Simply visit the URL of the form <code>https://vscode.dev/tunnel/frida-{job-name}[/{path-to-workspace}]</code>. If you wish to close the tunnel before the job times out press <code>Ctrl-C</code> in the terminal where you started the job.</p> <pre><code>ilb@login-frida:~$ stunnel -c32 --mem=48G --gres=gpu:1 --container-image=nvcr.io#nvidia/pytorch:23.10-py3 --job-name torch-debug\n# srun -p dev -c32 --mem=48G --gres=gpu:1 --container-image=nvcr.io#nvidia/pytorch:23.10-py3 --job-name torch-debug --pty code_tunnel\npyxis: importing docker image: nvcr.io#nvidia/pytorch:23.10-py3\npyxis: imported docker image: nvcr.io#nvidia/pytorch:23.10-py3\nTue Nov 28 21:31:40 UTC 2023\n*\n* Visual Studio Code Server\n*\n* By using the software, you agree to\n* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license) and\n* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n*\n\u2714 How would you like to log in to Visual Studio Code? \u00b7 Github Account\nTo grant access to the server, please log into https://github.com/login/device and use code 17AC-E3AE\n[2023-11-28 21:31:15] info Creating tunnel with the name: frida-torch-debug\n\nOpen this link in your browser https://vscode.dev/tunnel/frida-torch-debug/workspace\n\n[2023-11-28 21:31:41] info [tunnels::connections::relay_tunnel_host] Opened new client on channel 2\n[2023-11-28 21:31:41] info [russh::server] wrote id\n[2023-11-28 21:31:41] info [russh::server] read other id\n[2023-11-28 21:31:41] info [russh::server] session is running\n[2023-11-28 21:31:43] info [rpc.0] Checking /local/scratch/root/590/.vscode/cli/ana/servers/Stable-1a5daa3a0231a0fbba4f14db7ec463cf99d7768e/log.txt and /local/scratch/root/590/.vscode/cli/ana/servers/Stable-1a5daa3a0231a0fbba4f14db7ec463cf99d7768e/pid.txt for a running server...\n[2023-11-28 21:31:43] info [rpc.0] Downloading Visual Studio Code server -&gt; /tmp/.tmpbbxB9w/vscode-server-linux-x64.tar.gz\n[2023-11-28 21:31:45] info [rpc.0] Starting server...\n[2023-11-28 21:31:45] info [rpc.0] Server started\n^C[2023-11-28 21:37:45] info Shutting down: Ctrl-C received\n[2023-11-28 21:37:45] info [rpc.0] Disposed of connection to running server.\nTue Nov 28 21:37:45 UTC 2023\n\nilb@login-frida:~$\n</code></pre>"},{"location":"FRIDA/slurm/#keeping-interactive-jobs-alive","title":"Keeping interactive jobs alive","text":"<p>Interactive sessions are typically run in the foreground, require a terminal, and last for the duration of the SSH session, or until the Slurm reservation times out (whichever comes first). On flaky internet connections, this can become a problem, where a lost connection might lead to a stopped job. One remedy is to use <code>tmux</code> or <code>screen</code> which allows for running interactive jobs in detached mode.</p>"},{"location":"FRIDA/slurm/#batch-jobs","title":"Batch jobs","text":"<p>Once development, testing, and/or debugging is complete, at least in ML/AI training tasks, the datasets typically become such that jobs last much longer than a normal SSH session. They can also be safely run without any interaction and also without a terminal. In Slurm parlance these types of jobs are termed batch jobs. A batch job is a shell script (typically marked by a <code>.sbatch</code> extension), whose header provides Slurm parameters that specify the resources needed to run the job. For details of all available parameters consult the official Slurm documentation. Below is a very brief deep-dive introduction to Slurm batch jobs.</p> <p>Assume a toy MNIST training script based on Pytorch and Lightning, named <code>train.py</code>. train.py<pre><code>import os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics.functional import accuracy\nimport lightning.pytorch as L\nfrom pathlib import Path\n\n\nclass LitMNIST(L.LightningModule):\n    def __init__(self, data_dir=os.getcwd(), hidden_size=64, learning_rate=2e-4):\n        super().__init__()\n\n        # We hardcode dataset specific stuff here.\n        self.data_dir = data_dir\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        channels, width, height = self.dims\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        )\n\n        self.hidden_size = hidden_size\n        self.learning_rate = learning_rate\n\n        # Build model\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(channels * width * height, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, self.num_classes),\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return F.log_softmax(x, dim=1)\n\n    def training_step(self, batch):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        acc = accuracy(preds, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(\n                self.data_dir, train=False, transform=self.transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=4)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=4)\n\n\n# Some prints that might be useful\nprint(\"SLURM_NTASKS =\", os.environ[\"SLURM_NTASKS\"])\nprint(\"SLURM_TASKS_PER_NODE =\", os.environ[\"SLURM_TASKS_PER_NODE\"])\nprint(\"SLURM_GPUS_PER_NODE =\", os.environ.get(\"SLURM_GPUS_PER_NODE\", \"N/A\"))\nprint(\"SLURM_CPUS_PER_NODE =\", os.environ.get(\"SLURM_CPUS_PER_NODE\", \"N/A\"))\nprint(\"SLURM_NNODES =\", os.environ[\"SLURM_NNODES\"])\nprint(\"SLURM_NODELIST =\", os.environ[\"SLURM_NODELIST\"])\nprint(\n    \"PL_TORCH_DISTRIBUTED_BACKEND =\",\n    os.environ.get(\"PL_TORCH_DISTRIBUTED_BACKEND\", \"nccl\"),\n)\n\n\n# Explicitly specify the process group backend if you choose to\nddp = L.strategies.DDPStrategy(\n    process_group_backend=os.environ.get(\"PL_TORCH_DISTRIBUTED_BACKEND\", \"nccl\")\n)\n\n# setup checkpointing\ncheckpoint_callback = L.callbacks.ModelCheckpoint(\n    dirpath=\"./checkpoints/\",\n    filename=\"mnist_{epoch:02d}\",\n    every_n_epochs=1,\n    save_top_k=-1,\n)\n\n# resume from checkpoint\nexisting_checkpoints = list(Path('./checkpoints/').iterdir()) if Path('./checkpoints/').exists() else []\nlast_checkpoint = str(sorted(existing_checkpoints, key=os.path.getmtime)[-1]) if len(existing_checkpoints)&gt;0 else None\n\n# model\nmodel = LitMNIST()\n# train model\ntrainer = L.Trainer(\n    callbacks=[checkpoint_callback],\n    max_epochs=100,\n    accelerator=\"gpu\",\n    devices=-1,\n    num_nodes=int(os.environ[\"SLURM_NNODES\"]),\n    strategy=ddp,\n)\ntrainer.fit(model,ckpt_path=last_checkpoint)\n</code></pre></p> <p>To run this training script on a single node with a single GPU of type NVIDIA A100 40GB SXM4 one needs to prepare the following script (<code>train_1xA100.sbatch</code>). The script will reserve 16 vCPU, 32GB RAM, and 1 NVIDIA A100 40GB SXM4 for 20 minutes. It will also give the job a name, for easier finding when listing the partition queue status. As part of the actual job steps, it will start a <code>pytorch:23.08-py3</code> container, mount the current path, install <code>lightning:2.1.2</code>, and finally run the training, all with one single <code>srun</code> command. Without additional parameters, the <code>srun</code> command will use all of the allocated resources. The standard output of the script will be saved into a file named <code>slurm-{SLURM_JOB_ID}.out</code>. train_1xA100.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=mnist-demo\n#SBATCH --time=20:00\n#SBATCH --gres=gpu:A100\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=32G\n\nsrun \\\n  --container-image nvcr.io#nvidia/pytorch:23.08-py3 \\\n  --container-mounts ${PWD}:${PWD} \\\n  --container-workdir ${PWD} \\\n  bash -c 'pip install lightning==2.1.2; python3 train.py'\n</code></pre></p> <p>The Slurm job is then executed with the following command. It runs in the background without any terminal output, but its progress can be monitored by viewing the corresponding output file. <pre><code>ilb@login-frida:~/mnist-demo$ sbatch -p frida train_1x100.sbatch\nSubmitted batch job 600\n\nilb@login-frida:~/mnist-demo$ squeue\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n               600     frida mnist-de      ilb  R       0:35      1 axa\n\nilb@login-frida:~/mnist-demo-1$ tail -f slurm-600.out\npyxis: importing docker image: nvcr.io#nvidia/pytorch:23.08-py3\npyxis: imported docker image: nvcr.io#nvidia/pytorch:23.08-py3\nLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\nCollecting lightning==2.1.2\n  Obtaining dependency information for lightning==2.1.2 from https://files.pythonhosted.org/packages/9e/8a/9642fdbdac8de47d68464ca3be32baca3f70a432aa374705d6b91da732eb/lightning-2.1.2-py3-none-any.whl.metadata\n  Downloading lightning-2.1.2-py3-none-any.whl.metadata (61 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 61.8/61.8 kB 2.8 MB/s eta 0:00:00\n...\n</code></pre></p> <p>To run the same training script on more GPUs one needs to change the <code>SBATCH</code> line with <code>--gres</code> and add a line that defines the number of parallel tasks Slurm should start (<code>--tasks</code>). For example, to run on 2 GPUs of type NVIDIA H100 80GB HBM3 and this time save the standard output in a file named <code>{SLURM_JOB_NAME}_{SLURM_JOB_ID}.out</code>, one needs to prepare the following batch script. train_2xH100.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=mnist-demo\n#SBATCH --partition=frida\n#SBATCH --time=20:00\n#SBATCH --gres=gpu:H100:2\n#SBATCH --tasks=2\n#SBATCH --cpus-per-task=16\n#SBATCH --mem=32G\n#SBATCH --output=%x_%j.out\n\nsrun \\\n  --container-image nvcr.io#nvidia/pytorch:23.08-py3 \\\n  --container-mounts ${PWD}:${PWD} \\\n  --container-workdir ${PWD} \\\n  bash -c 'pip install lightning==2.1.2; python3 train.py'\n</code></pre></p> <p>Since the script also preselects the partition on which to run it is not necesary to provide it when executing the <code>sbatch</code> command. <pre><code>ilb@login-frida:~/mnist-demo$ sbatch train_2xH100.sbatch\nSubmitted batch job 601\n\nilb@login-frida:~/mnist-demo$\n</code></pre></p> <p>A more advanced script that allows for a custom organization of experiment results, autoarchival of ran scripts, can be run single-node single-GPU or multi-node multi-GPU, modifies the required container on the fly, and is capable of auto-re-queueing itself and continuation, can be seen in the following listing. The process of submitting the job remains the same as before. train_2x2.sbatch<pre><code>#!/bin/bash\n#SBATCH --job-name=mnist-demo # provide a job name\n#SBATCH --account=lpt # povide the account which determines the resource limits\n#SBATCH --partition=frida # provide the partition from where the resources should be allocated\n#SBATCH --time=20:00 # provide the time you require the resources for\n#SBATCH --nodes=2 # provide the number of nodes you are requesting\n#SBATCH --ntasks-per-node=2 # provide the number of tasks to run on a node; slurm runs this many replicas of the subsequent commands\n#SBATCH --gpus-per-node=2 # provide the number of GPUs that is required per node; avoid using --gpus-per-tasks as there are issues with allocation as well as inter-gpu communication - see https://github.com/NVIDIA/pyxis/issues/73\n#SBATCH --cpus-per-task=4 # provide the number of CPUs that is required per task; be mindful that different nodes have different numbers of available CPUs\n#SBATCH --mem-per-cpu=4G # provide the required memory per CPU that is required per task; be mindful that different nodes have different amounts of memory\n#SBATCH --output=/dev/null # we do not care for the log of this setup script, we redirect the log of the execution srun\n#SBATCH --signal=B:USR2@90 # instruct Slurm to send a notification 90 seconds prior to running out of time\n# SBATCH --signal=B:TERM@60 # this is to enable the graceful shutdown of the job, see https://slurm.schedmd.com/sbatch.html#lbAH and lightning auto-resubmit, see https://lightning.ai/docs/pytorch/stable/cloudsy/cluster_advanced.html#enable-auto-wall-time-resubmissions\n\n# get the name of this script\nif [ -n \"${SLURM_JOB_ID:-}\" ] ; then\n  SBATCH=$(scontrol show job \"$SLURM_JOB_ID\" | awk -F= '/Command=/{print $2}')\nelse\n  SBATCH=$(realpath \"$0\")\nfi\n\n# allow passing a version, for autorequeue\nif [ $# -gt 1 ] || [[ \"$0\" == *\"help\" ]] || [ -z \"${SLURM_JOB_ID:-}\" ] || [[ \"$1\" != \"--version=\"* ]]; then\n  echo -e \"\\nUsage: sbatch ${SBATCH##*$PWD/} [--version=&lt;version&gt;] \\n\"\n  exit 1\nfi\n\n# convert the --key=value arguments to variables\nfor argument in \"$@\"\ndo\n  if [[ $argument == *\"=\"* ]]; then\n    key=$(echo $argument | cut -f1 -d=)\n    value=$(echo $argument | cut -f2 -d=)\n    if [[ $key == *\"--\"* ]]; then\n        v=\"${key/--/}\"\n        declare ${v,,}=\"${value}\"\n   fi\n  fi\ndone\n\n# setup signal handler for autorequeue\nif [ \"${version}\" ]; then\n  sig_handler() {\n    echo -e \"\\n\\n$(date): Signal received, requeuing...\\n\\n\" &gt;&gt; ${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}/${RUN_NAME}.1.txt\n    scontrol requeue ${SLURM_JOB_ID}\n  }\n  trap 'sig_handler' SIGUSR2\nfi\n\n# time of running script\nDATETIME=`date \"+%Y%m%d-%H%M\"`\nversion=${version:-$DATETIME} # if version is not set, use DATETIME as default\n\n# work dir\nWORK_DIR=${HOME}/mnist\n\n# experiment name\nEXPERIMENT_NAME=demo\n\n# experiment dir\nEXPERIMENT_DIR=${WORK_DIR}/exp\nmkdir -p ${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}\n\n# set run name\nif [ \"${version}\" == \"${DATETIME}\" ]; then\n  RUN_NAME=${version}\nelse\n  RUN_NAME=${version}_R${DATETIME}\nfi\n\n# archive this script\ncp -rp ${SBATCH} ${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}/${RUN_NAME}.sbatch\n\n# prepare execution script name\nSCRIPT=${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}/${RUN_NAME}.sh\ntouch $SCRIPT\nchmod a+x $SCRIPT\n\nIS_DISTRIBUTED=$([ 1 -lt $SLURM_JOB_NUM_NODES ] &amp;&amp; echo \" distributed over $SLURM_JOB_NUM_NODES nodes\" || echo \" on 1 node\")\n\n# prepare the execution script content\necho \"\"\"#!/bin/bash\n\n# using `basename $SBATCH` -&gt; $RUN_NAME.sbatch, running $SLURM_NPROCS tasks$IS_DISTRIBUTED\n\n# prepare sub-script for debug outputs\necho -e \\\"\\\"\\\"\n# starting at \\$(date)\n# running process \\$SLURM_PROCID on \\$SLURMD_NODENAME\n\\$(nvidia-smi | grep Version | sed -e 's/ *| *//g' -e \\\"s/   */\\n# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\" -e \\\"s/^/# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\")\n\\$(nvidia-smi -L | sed -e \\\"s/^/# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\")\n\\$(python -c 'import torch; print(f\\\"torch: {torch.__version__}\\\")' | sed -e \\\"s/^/# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\")\n\\$(python -c 'import torch, torch.utils.collect_env; torch.utils.collect_env.main()' | sed \\\"s/^/# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\")\n\\$(env | grep -i Slurm | sort | sed \\\"s/^/# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\")\n\\$(cat /etc/nccl.conf | sed \\\"s/^/# \\${SLURMD_NODENAME}.\\${SLURM_PROCID}&gt;   /g\\\")\n\\\"\\\"\\\"\n\n# set unbuffered python for realtime container logging\nexport PYTHONFAULTHANDLER=1\nexport NCCL_DEBUG=INFO\n\n# train\npython /train/train.py\n\necho -e \\\"\\\"\\\"\n# finished at \\$(date)\n\\\"\\\"\\\"\n\"\"\" &gt;&gt; $SCRIPT\n\n# install lightning into container, ensure it is run only once per node\nsrun \\\n  --ntasks=${SLURM_JOB_NUM_NODES} \\\n  --ntasks-per-node=1 \\\n  --container-image nvcr.io#nvidia/pytorch:23.08-py3 \\\n  --container-name ${SLURM_JOB_NAME} \\\n  --output=\"${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}/${RUN_NAME}.%s.txt\" \\\n  pip install lightning==2.1.2 \\\n  || exit 1\n\n# run training script, run in background to receive signal notification prior to timout\nsrun \\\n  --container-name ${SLURM_JOB_NAME} \\\n  --container-mounts ${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}:/exp,${WORK_DIR}:/train \\\n  --output=\"${EXPERIMENT_DIR}/${EXPERIMENT_NAME}/${version}/${RUN_NAME}.%s.txt\" \\\n  --container-workdir /exp \\\n  /exp/${RUN_NAME}.sh \\\n  &amp;\nPID=$!\nwait $PID;\n</code></pre></p> <p>This script can then be run and monitored with the standard commands. <pre><code>ilb@login-frida:~/mnist-demo$ sbatch train_2x2.sbatch --version=v1\nSubmitted batch job 602\n\nilb@login-frida:~/mnist-demo$ tail -f exp/demo/v1/v1_20231213-2237.1.txt\n...\n</code></pre></p>"},{"location":"FRIKA/about/","title":"About FRIKA","text":"<p>FRIKA consists of three NVIDIA HGX Redstone GPU servers specifically dedicated to inferencing. The principal goal is to provide infrastructure for researchers and laboratories that want to offer their solutions (models/applications) as web services and thus promote their research/development work at UL FRI.</p> <p>FRIKA is currently running as a system of individual nodes that provide resources via Incus virtual machines and/or containers. Depending on requirements it is planned to be progressively further expanded. A long-term plan is to port all services to a Kubernetes-based cluster.</p> <p>Research labs that own and manage their inferencing systems themselves, may inquire about the possibility of integrating their infrastructure into FRIKA. Cofunding of future FRIKA expansions is also possible. All inquiries should be addressed to the UL FRI Management Board, the technical details will be coordinated by the FRIKA technical committee.</p>"},{"location":"FRIKA/access/","title":"Obtaining resources","text":"<p>Access to the FRIKA infrastructure may be granted (upon request) to all employees of UL FRI. Applications should be addressed to the UL FRI Management Board. They should briefly explain the services that are to be deployed on the allocated VM and the amount of resources needed (number of vCPUs, amount of VM memory, amount of memory on the GPU, amount of disk), together with a justification of the scope. The maximum amount of RAM per GPU requested is 40GB (systems are based on A100 40GB SMX4). The application should also list the expected level of utilization, i.e. the expected number of users, and how this fits in with the promotion of UL FRI. All websites and/or services deployed on FRIKA are expected to announce that they are running on the UL FRI FRIKA infrastructure (exposure of a UL FRI logo is sufficient).</p> <p>Once access is granted by the UL FRI Management Board, the technical questions should be directed to frika@rdc.si.</p> <p>Resource usage is monitored and in case of higher numbers of applications, the allocated quota may be reduced depending on utilization history.</p>"}]}